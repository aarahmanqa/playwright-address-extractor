name: Address Extraction with Sharding

on:
  workflow_dispatch:
    inputs:
      target_state:
        description: 'Target state(s) to process (e.g., "ALL", "CA", or "CA,NY,TX")'
        required: false
        default: 'ALL'
      total_shards:
        description: 'Number of parallel shards (1-50) - Use 42+ for 100% coverage with 1000 records/shard'
        required: false
        default: '45'
      max_records_per_shard:
        description: 'Maximum records per shard (500-2000)'
        required: false
        default: '1000'

env:
  NODE_VERSION: '18'
  PLAYWRIGHT_BROWSERS_PATH: 0

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      total_shards: ${{ steps.set-matrix.outputs.total_shards }}
    steps:
      - name: Set up matrix
        id: set-matrix
        run: |
          TOTAL_SHARDS="${{ github.event.inputs.total_shards || '45' }}"
          MAX_RECORDS="${{ github.event.inputs.max_records_per_shard || '1000' }}"
          TARGET_STATE="${{ github.event.inputs.target_state || 'ALL' }}"

          echo "total_shards=$TOTAL_SHARDS" >> $GITHUB_OUTPUT

          # Create matrix array using Python instead of jq
          python3 -c "import json; total_shards = int('$TOTAL_SHARDS'); matrix = list(range(total_shards)); print('matrix=' + json.dumps(matrix))" >> $GITHUB_OUTPUT

          echo "üîß Configuration:"
          echo "  - Total Shards: $TOTAL_SHARDS"
          echo "  - Target State: $TARGET_STATE"
          echo "  - Max Records per Shard: $MAX_RECORDS"
          echo "  - Expected Coverage: ~$((TOTAL_SHARDS * MAX_RECORDS)) records"

  extract-addresses:
    needs: setup
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hours timeout per shard
    strategy:
      fail-fast: false  # Continue other shards even if one fails
      matrix:
        shard_index: ${{ fromJson(needs.setup.outputs.matrix) }}
    
    env:
      SHARD_INDEX: ${{ matrix.shard_index }}
      TOTAL_SHARDS: ${{ needs.setup.outputs.total_shards }}
      MAX_RECORDS_PER_SHARD: ${{ github.event.inputs.max_records_per_shard || '1000' }}
      CI: true
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install chromium --with-deps

      - name: Configure target state
        run: |
          TARGET_STATE="${{ github.event.inputs.target_state || 'ALL' }}"
          echo "üéØ Configuring target state: $TARGET_STATE"

          # Verify files exist
          echo "üìÅ Checking files:"
          ls -la tests/MyScript.spec.ts
          ls -la FinalZipcodeState.csv

          # Update the test file with the target state
          if [ "$TARGET_STATE" != "ALL" ]; then
            echo "üîß Updating target state to: $TARGET_STATE"

            # Create a temporary file with the replacement
            cp tests/MyScript.spec.ts tests/MyScript.spec.ts.backup

            # Show current line before replacement
            echo "üìã Before replacement:"
            grep "const TARGET_STATE" tests/MyScript.spec.ts

            # Use a simple file replacement approach
            if [[ "$TARGET_STATE" == *","* ]]; then
              # Multiple states: convert "CA,NY,TX" to ["CA", "NY", "TX"]
              IFS=',' read -ra STATES <<< "$TARGET_STATE"
              FORMATTED_STATES=""
              for state in "${STATES[@]}"; do
                if [ -z "$FORMATTED_STATES" ]; then
                  FORMATTED_STATES="\"$state\""
                else
                  FORMATTED_STATES="$FORMATTED_STATES, \"$state\""
                fi
              done
              NEW_LINE="const TARGET_STATE: string | string[] = [$FORMATTED_STATES];"
            else
              # Single state
              NEW_LINE="const TARGET_STATE: string | string[] = \"$TARGET_STATE\";"
            fi

            # Replace the line using awk to avoid sed escaping issues
            awk -v new_line="$NEW_LINE" '
              /const TARGET_STATE: string \| string\[\] = '\''ALL'\'';/ { print new_line; next }
              { print }
            ' tests/MyScript.spec.ts.backup > tests/MyScript.spec.ts

            # Show line after replacement
            echo "üìã After replacement:"
            grep "const TARGET_STATE" tests/MyScript.spec.ts
          fi

      - name: Run address extraction (Shard ${{ matrix.shard_index }})
        run: |
          echo "üöÄ Starting shard ${{ matrix.shard_index }}/${{ needs.setup.outputs.total_shards }}"
          echo "üìä Processing up to ${{ env.MAX_RECORDS_PER_SHARD }} records in this shard"

          # Debug environment variables
          echo "üîç Environment variables:"
          echo "  SHARD_INDEX: ${{ matrix.shard_index }}"
          echo "  TOTAL_SHARDS: ${{ needs.setup.outputs.total_shards }}"
          echo "  MAX_RECORDS_PER_SHARD: ${{ env.MAX_RECORDS_PER_SHARD }}"
          echo "  CI: ${{ env.CI }}"

          # Test if we can list tests first
          echo "üìã Listing available tests:"
          npx playwright test tests/MyScript.spec.ts --list --reporter=line | head -10 || echo "Failed to list tests"

          # Run with increased workers and optimized settings for CI
          npx playwright test tests/MyScript.spec.ts \
            --workers=4 \
            --timeout=180000 \
            --retries=2 \
            --reporter=list \
            --output=test-results-shard-${{ matrix.shard_index }}

      - name: Upload shard results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: shard-${{ matrix.shard_index }}-results
          path: |
            FinalZipcodeState_shard_${{ matrix.shard_index }}.csv
            test-results-shard-${{ matrix.shard_index }}/
          retention-days: 30

      - name: Upload shard logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: shard-${{ matrix.shard_index }}-logs
          path: |
            playwright-report/
            test-results/
          retention-days: 7

  merge-results:
    needs: [setup, extract-addresses]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all shard results
        uses: actions/download-artifact@v4
        with:
          pattern: shard-*-results
          merge-multiple: true

      - name: Merge CSV results
        run: |
          echo "üîÑ Merging results from ${{ needs.setup.outputs.total_shards }} shards..."
          
          # Create merged file with header
          echo "zipcode,state,address,city" > FinalZipcodeState_merged.csv
          
          # Merge all shard files
          MERGED_COUNT=0
          for i in $(seq 0 $(({{ needs.setup.outputs.total_shards }}-1))); do
            SHARD_FILE="FinalZipcodeState_shard_${i}.csv"
            if [ -f "$SHARD_FILE" ]; then
              # Skip header and append data
              tail -n +2 "$SHARD_FILE" >> FinalZipcodeState_merged.csv
              SHARD_COUNT=$(tail -n +2 "$SHARD_FILE" | wc -l)
              MERGED_COUNT=$((MERGED_COUNT + SHARD_COUNT))
              echo "‚úÖ Merged shard $i: $SHARD_COUNT records"
            else
              echo "‚ö†Ô∏è Shard file $SHARD_FILE not found"
            fi
          done
          
          echo "üìä Total merged records: $MERGED_COUNT"
          echo "üìÅ Final file: FinalZipcodeState_merged.csv"

      - name: Generate summary report
        run: |
          echo "üìà Generating extraction summary..."
          
          TOTAL_RECORDS=$(tail -n +2 FinalZipcodeState_merged.csv | wc -l)
          SUCCESSFUL_RECORDS=$(tail -n +2 FinalZipcodeState_merged.csv | grep -v "Not Found\|Error" | wc -l)
          NOT_FOUND_RECORDS=$(tail -n +2 FinalZipcodeState_merged.csv | grep "Not Found" | wc -l)
          ERROR_RECORDS=$(tail -n +2 FinalZipcodeState_merged.csv | grep "Error" | wc -l)
          SUCCESS_RATE=$(echo "scale=2; $SUCCESSFUL_RECORDS * 100 / $TOTAL_RECORDS" | bc -l)
          
          cat > extraction_summary.md << EOF
          # Address Extraction Summary
          
          ## Configuration
          - **Target State(s)**: ${{ github.event.inputs.target_state || 'ALL' }}
          - **Total Shards**: ${{ needs.setup.outputs.total_shards }}
          - **Max Records per Shard**: ${{ github.event.inputs.max_records_per_shard || '1000' }}
          
          ## Results
          - **Total Records Processed**: $TOTAL_RECORDS
          - **Successful Extractions**: $SUCCESSFUL_RECORDS ($SUCCESS_RATE%)
          - **Not Found**: $NOT_FOUND_RECORDS
          - **Errors**: $ERROR_RECORDS
          
          ## Files Generated
          - \`FinalZipcodeState_merged.csv\` - Complete merged results
          - Individual shard files: \`FinalZipcodeState_shard_*.csv\`
          EOF
          
          cat extraction_summary.md

      - name: Upload final results
        uses: actions/upload-artifact@v4
        with:
          name: final-merged-results
          path: |
            FinalZipcodeState_merged.csv
            extraction_summary.md
            FinalZipcodeState_shard_*.csv
          retention-days: 90

      - name: Comment on workflow run
        if: github.event_name == 'workflow_dispatch'
        run: |
          echo "üéâ Address extraction completed!"
          echo "üìÅ Download the 'final-merged-results' artifact to get your complete dataset."
          echo "üìä Check the extraction_summary.md for detailed statistics."
